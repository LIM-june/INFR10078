lm_stats=[5579336, 0.3116466822608252, 0.9853501906482038, 1.4536908944718608e-06, 0.005158178014182952, 0.348077988641086]
best10_ents=[(2.4921691054394848, ['and', 'here', 'is', 'proof', 'the']), (2.5390025889056123, ['and', 'bailed', 'he', 'here', 'is', 'man', 'on', 'that', 'the']), (2.5584079236733106, ['is', 'the', 'this', 'weather', 'worst']), (2.5686534278173125, ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's']), (2.569853705187651, ['be', 'bus', 'here', 'the', 'to', 'want']), (2.576919752608039, ['hell', 'that', 'the', 'was', 'wat']), (2.587767243678531, ['creation', 'is', 'of', 'on', 'story', 'the', 'the']), (2.5885860368906832, ['fro', 'one', 'the', 'the', 'with']), (2.595298329492654, ['is', 'money', 'motive', 'the', 'the']), (2.617870705175611, ['at', 'bucks', 'end', 'lead', 'of', 'the', 'the', 'the'])]
worst10_ents=[(17.523736748003564, ['作品によっては怪人でありながらヒーロー', 'あるいはその逆', 'というシチュエーションも多々ありますが', 'そうした事がやれるのもやはり怪人とヒーローと言うカテゴリが完成しているからだと思うんですよね', 'あれだけのバリエーションがありながららしさを失わないデザインにはまさに感服です']), (17.524868750262904, ['ロンブーの淳さんはスピリチュアルスポット', 'セドナーで瞑想を実践してた', 'これらは偶然ではなく必然的に起こっている', '自然は全て絶好のタイミングで教えてくれている', 'そして今が今年最大の大改革時期だ']), (17.5264931699585, ['実物経済と金融との乖離を際限なく広げる', 'レバレッジが金融で儲けるコツだと', 'まるで正義のように叫ぶ連中が多いけど', 'これほど不健全な金融常識はないと思う', '連中は不健全と知りながら', '他の奴がやるから出し抜かれる前に出し抜くのが道理と言わんばかりに群がる']), (17.527615646393077, ['一応ワンセット揃えてみたんだけど', 'イマイチ効果を感じないのよね', 'それよりはオーラソーマとか', '肉体に直接働きかけるタイプのアプローチの方が効き目を感じ取りやすい', '波動系ならバッチよりはホメオパシーの方がわかりやすい']), (17.53293217459052, ['慶喜ほどの人でさえこうなんだから', '並の人間だったらなおさら参謀無しじゃ何も出来ない', '一般に吹聴されてる慶喜のネガティブ論は', 'こうした敵対勢力による相次ぐテロに対して終始無関心で', '慶喜個人だけに批判を向けがち']), (17.541019489814225, ['昨日のセミナーではお目にかかれて光栄でした', '楽しく充実した時間をありがとうございました', '親しみのもてる分かりやすい講演に勇気を頂きました', '素晴らしいお仕事とともに益々のご活躍願っております', '今後ともよろしくお願いします']), (17.541411086467402, ['自民党が小沢やめろというなら', '当然町村やめろというブーメランがかえってくるわけです', 'おふたりとも選挙で選ばれた正当な国民の代表ですから', 'できればどちらにもやめてほしくありません', 'そろそろこんな不毛なことはやめにしてほしい']), (17.5427257173663, ['知識欲というのは不随意筋でできている', 'どうせ人間には永久に解明できないんだから', '宇宙はある時点で生まれたのか', 'それとも永遠の過去から存在しているのかなんてことを追究するなと言ってもムダだ', '心臓に止まれと命令しても止まらないのと同じことだ']), (17.547644050965395, ['と言いつつもやっぱり笑えない時はあるよなあ', '笑っても自分の笑顔が汚らわしく思えてすぐ止めちゃうの', '自分が息してるだけで悲しくてぼろぼろ泣いてる時期もあった', '今の自分に必要な経験だったとは思うけど', '出来ればあんな感情は二度とごめんだ']), (17.55280652132174, ['中身の羽毛は精製過程で殺菌処理しているから', '羽毛布団からダニが湧くことはない', 'あと羽毛布団の生地は糸の打ち込み本数が多く', '羽毛の吹き出しを防ぐ目つぶし加工をしているからダニは羽毛ふとんの生地を通過できない', 'ただダニが布団に付着することはあるから手入れは必要'])]
answer_open_question_3="The words at the beginning of the list are mostly just single letters and have lower entropies. Frequently used articles like 'a' and 'the', conjunctions like 'and', and short verbs like 'is' appear a lot of times.\nMeanwhile, the words at the end of the list have a much longer length, non-English words, and are forms of connected words without a space in between.\nThey are not frequent in the corpus because they are not the normal usage of the words, and that leads to having higher entropies."
answer_open_question_4='1. Fix spellings or spaces by computing MED(minimum edit distance) via dynamic programming and apply costs for the computation by EM(Expectation Maximisation) Algorithm.\n2. Eliminate non-English words by applying .isascii() method to the words, since they contain only American letters and some punctuations(these are already eliminated by the .isalpha() method)'
mean=3.8435755769050926
std=0.47772976561662
best10_ascci_ents=[(2.4921691054394848, ['and', 'here', 'is', 'proof', 'the']), (2.5390025889056123, ['and', 'bailed', 'he', 'here', 'is', 'man', 'on', 'that', 'the']), (2.5584079236733106, ['is', 'the', 'this', 'weather', 'worst']), (2.5686534278173125, ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's']), (2.569853705187651, ['be', 'bus', 'here', 'the', 'to', 'want']), (2.576919752608039, ['hell', 'that', 'the', 'was', 'wat']), (2.587767243678531, ['creation', 'is', 'of', 'on', 'story', 'the', 'the']), (2.5885860368906832, ['fro', 'one', 'the', 'the', 'with']), (2.595298329492654, ['is', 'money', 'motive', 'the', 'the']), (2.617870705175611, ['at', 'bucks', 'end', 'lead', 'of', 'the', 'the', 'the'])]
worst10_ascci_ents=[(5.166314124571327, ['hoje', 'nossa', 'amiga', 'espero', 'q', 'sorte', 'tenha', 'vc']), (5.166378486661663, ['aok', 'berlin', 'brandenburg', 'bürofläche', 'commercial', 'engel', 'immobilien', 'meldung', 'mietet', 'potsdam', 'potsdam', 'qm', 'v', 'völkers']), (5.166607294898407, ['mi', 'rt', 'ixxi', 'squeciduu', 'yinha']), (5.166636591109558, ['asdhiasdhiuadshiuads', 'rt', 'tentaando', 'to', 'x']), (5.166680391667252, ['aaaaaai', 'aqui', 'e', 'é', 'gente', 'guri', 'horror', 'lente', 'não', 'não', 'o', 'olho', 'que', 'que', 's', 'tem', 'tem', 'um', 'vermelho']), (5.166696700850563, ['aaaon', 'aah', 'as', 'cow', 'da', 'eu', 'lindas', 'parade', 'vaquinhas', 'viii']), (5.16672881298096, ['bra', 'di', 'douglas', 'douglas', 'ett', 'finansmannen', 'för', 'för', 'gustaf', 'gustaf', 'konkurrensen', 'och', 'skogsägaren', 'slag', 'slår', 'sveaskog']), (5.166820287702661, ['bola', 'macaé', 'rolando', 'vasco', 'x']), (5.1670133043123725, ['enaknya', 'hari', 'hmmm', 'ini', 'kmn', 'ya']), (5.16719955611981, ['ad', 'emg', 'ha', 'haha', 'ak', 'jg', 'k', 'kreta', 'krta', 'ksmg', 'mau', 'mba', 'naik', 'rt', 'smg', 'wkwk'])]
best10_non_eng_ents=[(4.321320405509358, ['afganistán', 'asociación', 'de', 'de', 'mujeres', 'rawa', 'revolucionarias']), (4.321322108677053, ['carrey', 'face', 'feat', 'mariah', 'minaj', 'my', 'nicki', 'out', 'rt', 'up', 'video', 'xxlmag', 'com']), (4.321338322311484, ['abisss', 'aja', 'demo', 'gini', 'hari', 'hikmah', 'mantabsss', 'membawa', 'sepi', 'sudirman', 'thamrin', 'tiap', 'trnyta']), (4.321374586541906, ['a', 'a', 'agora', 'com', 'consegui', 'd', 'de', 'dormir', 'dormir', 'durmo', 'e', 'eu', 'inteira', 'mas', 'nao', 'nao', 'nao', 'nao', 'noite', 'noite', 'nove', 'q', 'se', 'sono', 'to', 'vou']), (4.321385367081537, ['eh', 'meu', 'o', 'que', 'twitter', 'esse']), (4.321390995790845, ['don', 't', 'get', 'giggle', 'i', 'i', 'oh', 'oh']), (4.321500139376771, ['am', 'geburtstag', 'gefeiert', 'klingende', 'november', 'töne', 'wird']), (4.321525080235015, ['attraktion', 'belønning', 'fest', 'kærlighed', 'lykke', 'privacy', 'succes', 'tarot', 'tillid', 'udholdenhed']), (4.321575437028815, ['cerca', 'de', 'de', 'deci', 'del', 'exameeen', 'examen', 'le', 'mateeee', 'matematica', 'pueda', 'que', 'rt', 'shhuu', 'shuuu', 'sientese', 'valla', 'wayyy', 'y']), (4.321622845063304, ['abre', 'china', 'huaehuiaieh', 'o', 'olho'])]
worst10_non_eng_ents=[(5.166314124571327, ['hoje', 'nossa', 'amiga', 'espero', 'q', 'sorte', 'tenha', 'vc']), (5.166378486661663, ['aok', 'berlin', 'brandenburg', 'bürofläche', 'commercial', 'engel', 'immobilien', 'meldung', 'mietet', 'potsdam', 'potsdam', 'qm', 'v', 'völkers']), (5.166607294898407, ['mi', 'rt', 'ixxi', 'squeciduu', 'yinha']), (5.166636591109558, ['asdhiasdhiuadshiuads', 'rt', 'tentaando', 'to', 'x']), (5.166680391667252, ['aaaaaai', 'aqui', 'e', 'é', 'gente', 'guri', 'horror', 'lente', 'não', 'não', 'o', 'olho', 'que', 'que', 's', 'tem', 'tem', 'um', 'vermelho']), (5.166696700850563, ['aaaon', 'aah', 'as', 'cow', 'da', 'eu', 'lindas', 'parade', 'vaquinhas', 'viii']), (5.16672881298096, ['bra', 'di', 'douglas', 'douglas', 'ett', 'finansmannen', 'för', 'för', 'gustaf', 'gustaf', 'konkurrensen', 'och', 'skogsägaren', 'slag', 'slår', 'sveaskog']), (5.166820287702661, ['bola', 'macaé', 'rolando', 'vasco', 'x']), (5.1670133043123725, ['enaknya', 'hari', 'hmmm', 'ini', 'kmn', 'ya']), (5.16719955611981, ['ad', 'emg', 'ha', 'haha', 'ak', 'jg', 'k', 'kreta', 'krta', 'ksmg', 'mau', 'mba', 'naik', 'rt', 'smg', 'wkwk'])]
answer_open_question_6='1. Different corpus has different backgrounds, which means that they have different styles of language usage, for example, tweet texts and newspaper articles differ a lot.\n=> Assume that the given corpus represents the entire English.\n\n2. There are much more words that are not used in the given corpus.\n=> Assume that the given corpus contains every existing English word.\n\n3. It is not sure how many words you need to predict the next word.\n=> Assume that n-1 words will be sufficient to predict the next word that comes for the n-gram model. \n\n\nTo solve the sparse data problem in #2 above, I would apply the appropriate smoothing method to my model.\nAlso, to choose which n to apply for the n-gram, I would compute per word entropy for different models like bigram, trigram, to 6-gram model and compare each entropy and pick the one with the lowest entropy value.        '
naive_bayes_vocab_size=13521
naive_bayes_prior={'V': 0.47766934282005674, 'N': 0.5223306571799433}
naive_bayes_likelihood=[0.0069130647433716524, 0.0012190937826220337, 0.12333945519183054, 2.231540142060584e-06, 2.6766530157370005e-05, 0.004917741586185889, 0.004933935254095951]
naive_bayes_posterior=[{'V': 0.5886037204340953, 'N': 0.41139627956590474}, {'V': 0.15633267093792835, 'N': 0.8436673290620716}, {'V': 0.8124219037837144, 'N': 0.18757809621628563}, {'V': 0.8124219037837144, 'N': 0.18757809621628563}, {'V': 0.9961437486715602, 'N': 0.0038562513284398038}]
naive_bayes_classify=['V', 'N']
naive_bayes_acc=0.7949987620698192
answer_open_question_8="The accuracy of the top four features on the list is interpreted as which position's word gives the most confidence on PP decision.\nAlso, the last feature is word sets with position labeled, so that feature is determining how likely a particular word set will give you a VP or NP.\nIt has similar accuracy with Naive Bayes because both methods train the whole word groups, but the logistic regression model has a slightly higher accuracy because the order is fixed, giving slightly more information."
lr_predictions='VVVVVVVVVVVVVVVVVVVVVVVVVVVVVNVVVVVVVVNVNVVVNVNNNNVVNVVVNNVNNNVNNVVVNNVVVNNNVVNVNVNVNNNVVVNVNNNVNVNVNNVNVVVVNNNNVNNNNVVVNVVNVNNVVNVNNVNVNNNVVNVNNVNNNNVNNNNVVNNNNVNNNNVNNNNNVVNNVVVVVVVNVNVNNNNVVVVNVVNVVVNVVNNVVNVNVNNNNVNVVVNVNVNNNNVNVNVNNVNNVNNVNNVNVVNNNNVNVNVNNVNVVNNNNVVNVNVNVVNVVVNVVNVNNVNNVVNNNVNNVVNNNNNVNVNNVVVNNNVVVVNVVVVVNNNNNNVVVNNNNNVNVNVNNVVNVNNNNVVNVNVVNVVVNNNVVNNVVVVVVNNNNNNVVNVVNNNNNNVNNVVVNNVVVVVVVVNNNVNVNVVNNNVVVVVVVVVNVVVVNVNVNNVVNVVVNVNNNNNVVNVNNNNNVNNNVVNVNNVNNNVNNNVNVNVVVVVVNVVNVNNVNNVNNVVVNVNNVVVVVVVVVVVVVVVVVNVVVNVNNVVNVVNNNNVNVNNNVNVNVNVVNNNNNNVVNNNNVVVVVNNVNVNNNNVNVVNVNVVNVNVVVVVVVNVNNNVVNNVVVNNNNVNVVNVVNVNNVVNVVVVNVVVNVNVVVVNNVNVVNVVNVVNNNNVVNNVNVNVVNNVVVVVVNVVVVVVVVVVVVVVVVNVNVVNNVVVVVVVVNNNNNVVNNNNVNNNNVVNVNVVVVNVNVVNVVVNVNNNNNNNVVNVNVNNVVVVNNNNNNVVVNVVVVVVVVNVNVNVVVNVNVVNVNNNVVNVNNNVNVNVNVVNVVNVVNNNNVNNVVNVVVNVVVVNVNVVNVNNNNVNNNVNVNVNNNNVVNNVNNVVVVVNVNNNNVNNNNVNVVVNNNNNVVNVVNNNVNNVNNNNNNVNNNVNVVVNVVVNVVNNVNNNVVNVVVVNVNNNNNVVVNNNVNNVNNNNNNNVVVVNVVNVVVVVNVVVVVVNNVVVNNNVVVNVVNNVNNNNNNNNVVNNNNVVVVVNVNVNVVVNNVNNVNVVNNNVNVNVNNVNVNVVVVVVVNVVNVNNVVVNNNNNVNNNNVNVNNVNNNNNNVVNNVVNNNNNNVVVVVNVNNNVNNNNNVVVVVVVVVNVVVNVNNVNVVNVNVVVNVVNVVVNVVVVVNNNNNNNNNNNNNNVNNVNNNVNNVNNVVVVNVVNVNNNNVVNVVNNNVNNNVVVNNNNVVNNNVNNNVNNNNVVVNVVNVNNNVVVNVNNVVNVVVNVNVNNNNVVNNNNNVVNNVNNNNNNVVVVNNVVVNVNVNNVVVNVNNVVVNVVNVVNVNNVVNNVNVVVNVVVNVVVVVVVVNVVNNNNVNVVNNNVVNNVNNVNVNNVVVNVNNNVVVNNVVVVNNNNVNVNNVVVVVNNNNNNVNVVNVNVVVVVNVNNNNVVNVVVNNVNVVNNVNNNNNNNNVNNNVNNNNNNVNVNNVNVVVNVVNNVNNVVNNNNNNVNNVVNNNVVVNNVNNNVVVNNNNVNNVNVNNNVNNVNNVNNVVNVNNVNNVNNNNVVNVNVVNNVNVNVNVNNVVVVVVNNVNNVNNVVVVNNNVVVVNVNVNVVVNNVNNNNNNVNVNVNNVVNNVNNNVNVVVNNVNNNVNVNVNNVVNVVNVVNNVNNVVVNNNNVVNNVVNNNVNVNNNVVNVNNVNVNVVNNVNVVNVVNVVVNNNVNNNVNNNNNNNNNNNNNVVVNVVVNVVVNVVNVVNVNVVVNNVVNNNNVVNVNVNVNVNVNVVVNVNNNNVVNVNNVNNVNNNNNNNVNVNVNNVNNVVVVNVVVNNNNNVNVNNNVNVNNVVNNNNNVNNNNNVNNVNVVNVNNVNNNVVVNVNVNVVNNVNVNVVVVVNNNVNNNNVVNVNNNNVNVVNNNNNNVNVNVVVVNVVNNNNVNNVNVVNNNNNVVNNVNNNNNNVNNVVNNVNNNNVNNVNVNVVVVNVNVNVNVVVVNVVVVVNVNVNVNVNNNVVVVNVNVNNVVNNNVNNNNNNVNVNNVVNNVNVNNNNNNVNVVVNNNVVVNNNVVVVVNVVVNNVNVNNVVNNVVVNVVNNVNVVVNVVVNVVVVNNVVVNNVVNNVVVVNNNNNVNVVNNNNNNVNVVVNVVNVNVVNNNVNNVNVNVVNNVVVVNNNVVVNNVNVVNNVNNVVVNVNVNVNVNNVVVVVVVVNNVVNVNNNVNVNNNVNNVVVNNNNVNNVVNNVVNVNNVVNVVNVNNVNVVVNNNVNVVVNVNNVVVVNNNNVVVVVNVVNVVVNNNVVVNVVNNVVVNNNVNNNNVNNNNVVVVNNVVVNVNNVNVVNVVNVNVNNVVVVNNVNVNVVVVVNVNNVVNVNNNNNVVNNVNNNVNNNNNVNVVVNVVVNNVVNNNNNVVVNNNVNVNNVVNVVNNVNVNNNNNVVVVNNNNVNNVNNVVVVVNVNVVVVVVVNNNNNNNVNNVVVVNVNVNVNNVVVNNNVNNNNVNNVVNNVNVNNNNVVVNVVNVNVNNVVNVNVVNVNVVVVVNVNVVVNNVVVVNNNVVVVNNVVVNNNVVVNNVVVNNNNNVVVNVNVVVNNVNNVVNVVVNNNNVNVVVVVVVVVNNVNVVNNNNNNVVVNVVVNNNNVVVNVNNVNNNNNVVVNVVNNVNVNNVNVVVVNVNVVVVVVVVNVNNNNNVVNVNVNVVNNVVNVNNVNVNNNNNNVVNNNVVVVVNNNNNVVVNNVNVVVVVVNNVNNVVNNVNVVNNNNVNNNVNNVVVVVNVVVNNVVVVVVNNVNNNVVVNVVVVVVVVNNVNNNVVNNVNVVVVVVNNVVVNVVVNVVVVNVVNNNVNVNVVVVNVNVVNNNVVVNVVVNNNVVNNVNNVNVNVNNVNVNNNNNVNVNVVVVNNNNNNNNNVNVNNVNVNNNNNVNVVNNNNVVVVVNVNNNVVVNVNVVVVVNNNNNNNVVVVVVNVVVVVNVVVVVNNVVNNVVNNVVVNNNNNNVVNVNNVVVNNNNVVNNVNNVVVVVVVNVVVNVNVNVNVVVVNVNVNNNVNVVNNNNNVVVNVVVNNVNNVNNNVNVNNNNNNNVVVVNVNVNVVVVVVVVVVVNVVVVVNNNVNNVNNVVVVNNVNVNNVVVVNNNNVVVVVVNNNNNVNVNNNVNNNVVNVNVVVVVNNVNVNNVNNVNNNNNVVVVVVVVVNVVVVNVVNNVNVNNNVNVNVVVVNVNNNVVVNNVVNNNNNNVVVNVNNVNNNVVNNNVVVVNVVNVVNNNVVNVVNVNVNNNNNNNVNNVNNVVVVNNVNVNNNVNVNVNVVVVVNVNVNNVNNVNNVVVVNNVNNVVVVVVNVNVVVVVVNVNVVVVNNVVVNNVVNNNNVVNNNVNVVNNNVVVNVNNNNVNNNVVVVVVVVVVNVVNNNNNNVVVNVVNNNVNVNNNNNNNVVNVNNNNVVNNVVNNNNVVVNVVNNNVNNVVVVNNVNNVNVVNVVNNNNVNVVNNNNVVVNNNVVNVVNNNNVVVVVVVNVVNNVVNNNNVVVVVVNNVVVVNNVNNVNVVNVVNVVNVVNNNNNVNVVNVNVVVNVNNNVVVVVVNVVVNNVNVNNNNNNVNVVNVVNVVVVVVNNNNNVVNNNNVVVVVVNNNVNVVNNNVVVNNNVVNNVVNVVNVVVNNVNNVVVVVNVVNNVNNVVVVVNVNNVVVNNNNNNVVNNNNVVVNNNNVVNNVNNVNVNVVVVNNNNVNVVNNNNNVNNVVNVVVNNNVVNNNNVVNNNVNVNVNVNVNVVVVVNNNNVVNVNNNVNNNVNNNNNNNVNVVVVNNNVNNNNNNNNVVNVNVVNVVNVNNNVVNNNNNNNVNVNNNVNVNVNVNVNVNNVNVNVNVNVNVNVVNNVNVNNNNNVNNNNNNVNVVVVNNVNNNNNNNNVNNVNNVVNNVNVVVVVNVVNNNVVNVVVNNVVNVNVNVNVVNVVNNVNVVVVVVNNNNNNNNNVNVNNNVNNNNNVNNVNVNNNVVVVNVNVNVNVNVNVVVNNNNNNVVVVNNNNNNNNVVNVVVNNVVNVVVNVVNNVVVVNNVNVNVNNNNVVNVNNVVVVVVNVVNNVNVNNVN'
answer_open_question_9="In addition to using every single position as a feature, I used all of the combinations of two positions as features because it enables you to predict which word phrases are used in the sentence, giving a more reliable hint on guessing the preposition's usage.\nPreposition word itself obviously is the biggest hint on predicting the result for example, 'of' or 'without'. \nPreposition combined with other positions, especially with verb ranks the top on the list, such as 'rose to' and 'fell to'.\nThese phrases reflect the context of the sentence and let you easily assume the next word, which also makes it easier to predict the sentence structure."

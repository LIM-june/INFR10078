a1aa=['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X']
a1b=2649
a1c=12.061525956381585
a1d='function'
a2a=13
a2b=2.4630366660416674
a3c=2.883894963080882
a3d='DET'
a4a3=0.8941143860297515
a4b1=[("I'm", 'PRT'), ('useless', 'ADJ'), ('for', 'ADP'), ('anything', 'NOUN'), ('but', 'CONJ'), ('racing', 'ADJ'), ('cars', 'NOUN'), ('.', '.')]
a4b2=[("I'm", 'PRT'), ('useless', 'ADJ'), ('for', 'ADP'), ('anything', 'NOUN'), ('but', 'ADP'), ('racing', 'VERB'), ('cars', 'NOUN'), ('.', '.')]
a4b3="1. P('CONJ'|'NOUN') is much higher than P('ADP'|'NOUN').\n2. P('racing'|'ADJ')*P('NOUN'|'ADJ') is much higher than P('racing'|'VERB')*P('NOUN'|'VERB')."
a4c=60.73342283709657
a4d=70.21922688308214
a4e=['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADV']
a5t0=0.558440358495796
a5tk=0.6090732698882011
a5b="'he' is nowhere in the original dataset, so this new word made P('he'|'PRON') change largely from 0.0001 to 0.0455, yielding new possibilities happen.\nSo the effect of introducing new case is alike smoothing.\n\nT_k mislabels 'them' as a noun while T_0 correctly labels it as 'PRON'. Since it is labeled only as 'PRON' in the original dataset, the coincidental new case makes P('them'|'NOUN') increase highly, leaving it stick to that labeling and hardly change."
a6='1. Use a trigram or higher model instead of bigram model.\n2. Give a higher smoothing value to increase the flexibility of seeing a new word.\n3. Give a priority to the transition probability than emission probability by giving a weight when you combine two probabilities in the tagging process.\n4. Assign an average probability as a default value for unseen words.'
a7="1. Smaller size of the tagset makes the number of the data for each tags higher. Since the train dataset is small, we need to use smaller tagset to get enough number of data for each categories to get meaningful accuracy.\n2. Accuracy increases because it's less ambiguous between different categories thus easier to fall in larger boundaries.\n3. Memory and space size decreases since there are less number of keys in the frequency or probability distribution dictionary."
a4full_vit=[[26.782208935674642, 27.175612695180405, 26.83747232781355, 25.69387747486122, 26.069930312266184, 25.33122669020975, 26.602772790690192, 26.7617409338053, 6.7157647918435055, 26.154747513228394, 28.346102791874564, 27.06281489027102], [34.01526253042612, 26.139704758753695, 34.54383920395947, 32.31894404945449, 34.25048137679512, 36.22833529220789, 27.617716596488677, 37.174460980126284, 34.96569792142534, 33.2167214529358, 30.719087535156927, 43.87771626842389], [52.96761801645391, 52.61690686578734, 53.24942252459456, 54.723565975137475, 52.53968502182914, 34.01218757982337, 51.358458806319945, 52.70225028680407, 55.14157828038837, 53.01419324341059, 54.12151433519213, 54.72321304758704], [64.02174333349448, 58.59624615156186, 64.37034195590547, 61.740806181353804, 67.52308363114037, 64.95730590909109, 44.113599640606985, 60.603762131928086, 62.65031911857109, 64.14971027238593, 61.85439770355394, 60.37766863378028]]
a4full_bp=[['PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON'], ['NOUN', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADJ'], ['DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET']]
